{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train an agent\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.autograd as autograd         \n",
    "import torch.nn as nn                     \n",
    "import torch.nn.functional as F           \n",
    "import torch.optim as optim               \n",
    "import torch.distributions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "\n",
    "import streamz\n",
    "import streamz.dataframe\n",
    "\n",
    "import pdb, gym\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training progress output\n",
    "def init_training_progress():\n",
    "\n",
    "    max_length = 1000000\n",
    "    rolling_size = 25\n",
    "\n",
    "    training_stream = streamz.Stream()\n",
    "    example = pd.DataFrame({'x': [0]}, index=[0])\n",
    "\n",
    "    training_sdf = streamz.dataframe.DataFrame(training_stream, example=example)\n",
    "\n",
    "    training_raw_buffer = Buffer(training_sdf, length=max_length)\n",
    "    training_smooth_buffer = Buffer(training_sdf.x.rolling(rolling_size).median())\n",
    "\n",
    "    training_raw_dmap = hv.DynamicMap(hv.Curve, streams=[training_raw_buffer]).relabel('raw')\n",
    "    training_smooth_dmap = hv.DynamicMap(hv.Curve, streams=[training_smooth_buffer]).relabel('smooth')\n",
    "    \n",
    "    return training_stream, training_raw_dmap, training_smooth_dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer of action means\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Standard deviations approximated seperately\n",
    "        self.register_parameter('log_sigma', None)\n",
    "        self.log_sigma = nn.Parameter(torch.zeros(action_size), requires_grad=True)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        \n",
    "        means = torch.tanh(self.fc3(x))\n",
    "        sigmas = torch.exp(self.log_sigma).expand(means.shape)\n",
    "        \n",
    "        return means, sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNet, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer - single state value\n",
    "        self.fc3= nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr=1e-3):\n",
    "        self.lr = lr\n",
    "        self.actor = PolicyNet(state_size, action_size)\n",
    "        self.critic = ValueNet(state_size)\n",
    "        self.actor_optimiser = optim.Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optimiser = optim.Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def act(self, state):\n",
    "        \n",
    "        # Derive action distribution from the policy network (actor)\n",
    "        means, sigmas = self.actor(state)\n",
    "        action_distribution = torch.distributions.Normal(means, sigmas)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_prob = action_distribution.log_prob(action)\n",
    "        \n",
    "        return state, action, action_log_prob\n",
    "    \n",
    "    def record_outcome(self, state, action, action_log_prob, reward, next_state):\n",
    "        \n",
    "        # Record reward for assesment\n",
    "        self.episode_rewards.append(reward)\n",
    "        \n",
    "        # Use the critic to estimate state values\n",
    "        state_value = self.critic(state).squeeze()\n",
    "        next_state_value = self.critic(next_state).squeeze()\n",
    "        \n",
    "        # Calculate the TD target/error and detach from the computation graph\n",
    "        td_target = (reward + next_state_value).detach()\n",
    "        td_error  = (reward + next_state_value - state_value).detach()\n",
    "        \n",
    "        # Train the agent\n",
    "        self.train_critic(td_target, state_value)\n",
    "        self.train_actor(td_error, action_log_prob)\n",
    "\n",
    "    def train_critic(self, td_target, state_value):\n",
    "        \n",
    "        loss = F.mse_loss(td_target, state_value)\n",
    "        \n",
    "        self.critic_optimiser\n",
    "        loss.backward()\n",
    "        self.critic_optimiser.step()\n",
    "    \n",
    "    def train_actor(self, td_error, action_log_prob):\n",
    "        \n",
    "        loss = -(td_error * action_log_prob.sum(dim=1)).mean()\n",
    "        \n",
    "        self.actor_optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optimiser.step()\n",
    "\n",
    "    def average_episode_return(self):\n",
    "        return sum([r.mean().item() for r in self.episode_rewards])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_and_learn(env, agent, max_episode_length=1000):\n",
    "    \n",
    "    # Run concurrent episode on all environments\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = torch.from_numpy(env_info.vector_observations).float()\n",
    "    \n",
    "    # Start episode\n",
    "    agent.start_episode()\n",
    "    \n",
    "    for _ in range(max_episode_length):    \n",
    "        \n",
    "        # Calculate actions for all envs\n",
    "        state, action, action_log_prob = agent.act(state)\n",
    "        \n",
    "        # Run through the envs in parallel\n",
    "        env_info = env.step(action.numpy())[brain_name]\n",
    "        next_state = torch.from_numpy(env_info.vector_observations).float()\n",
    "        reward = torch.tensor(env_info.rewards).float()\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        # Record the experience tuple with the agent\n",
    "        agent.record_outcome(state, action, action_log_prob, reward, next_state)\n",
    "        \n",
    "        # Advance\n",
    "        state = next_state\n",
    "\n",
    "        # We want rectangular input to network, so if any finish early we finish all early\n",
    "        if np.any(done):\n",
    "            print(\"Someone finished\")\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and agent\n",
    "state_size = brain.vector_observation_space_size\n",
    "action_size = brain.vector_action_space_size\n",
    "agent = Agent(state_size, action_size, lr=1e-3)\n",
    "episode_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress\n",
    "stream, smooth, raw = init_training_progress()\n",
    "layout = (smooth * raw)\n",
    "layout.opts(\n",
    "    opts.Curve(width=900, height=300, show_grid=True, tools=['hover'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "num_episodes = 5\n",
    "max_episode_length = 1000\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    \n",
    "    play_episode_and_learn(env, agent, max_episode_length=max_episode_length)\n",
    "    \n",
    "    average_episode_return = (agent.average_episode_return() / max_episode_length) * 1000\n",
    "    stream.emit( pd.DataFrame({'x': average_episode_return}, index=[episode_index]) )\n",
    "    episode_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show params\n",
    "for x in agent.subagents[0].policy.parameters():\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent in the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]      # reset the environment\n",
    "states = torch.from_numpy(env_info.vector_observations).float()\n",
    "num_agents = len(env_info.agents)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "for i in range(100):\n",
    "    actions = agent.act(states)\n",
    "    env_info = env.step(actions.numpy())[brain_name]\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    states = torch.from_numpy(env_info.vector_observations).float()\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "env_info.vector_observations.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "torch.save(agent.policy, 'agent.policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "agent.policy = torch.load('agent.policy')\n",
    "agent.policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "state_size = brain.vector_observation_space_size\n",
    "action_size = brain.vector_action_space_size\n",
    "agent = Agent(state_size, action_size, lr=3e-4, rollout_length=3)\n",
    "\n",
    "agent.start_episode()\n",
    "\n",
    "# Dummy interact with env\n",
    "states = np.zeros((20,33))\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    # Get actions\n",
    "    pdb.set_trace()\n",
    "    actions = agent.act(torch.from_numpy(states).float())\n",
    "    \n",
    "    # Dummy interact with env\n",
    "    next_states = (i+1) * 0.1 * np.ones((20,33))\n",
    "    rewards = 0.1 * np.ones(20)\n",
    "    dones = [False] * 20\n",
    "\n",
    "    # Teach agent\n",
    "    agent.record_outcome(rewards)\n",
    "    \n",
    "    # Advance\n",
    "    states = next_states\n",
    "\n",
    "agent.finish_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "agent = Agent(state_size, action_size, lr=3e-4, rollout_length=3)\n",
    "for x in agent.policy.parameters():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
