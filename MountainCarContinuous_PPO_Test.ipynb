{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.autograd as autograd         \n",
    "import torch.nn as nn                     \n",
    "import torch.nn.functional as F           \n",
    "import torch.optim as optim               \n",
    "import torch.distributions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "\n",
    "import streamz\n",
    "import streamz.dataframe\n",
    "\n",
    "import pdb, gym\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training progress output\n",
    "def init_training_progress():\n",
    "\n",
    "    max_length = 1000000\n",
    "    rolling_size = 25\n",
    "\n",
    "    training_stream = streamz.Stream()\n",
    "    example = pd.DataFrame({'x': [0]}, index=[0])\n",
    "\n",
    "    training_sdf = streamz.dataframe.DataFrame(training_stream, example=example)\n",
    "\n",
    "    training_raw_buffer = Buffer(training_sdf, length=max_length)\n",
    "    training_smooth_buffer = Buffer(training_sdf.x.rolling(rolling_size).median())\n",
    "\n",
    "    training_raw_dmap = hv.DynamicMap(hv.Curve, streams=[training_raw_buffer]).relabel('raw')\n",
    "    training_smooth_dmap = hv.DynamicMap(hv.Curve, streams=[training_smooth_buffer]).relabel('smooth')\n",
    "    \n",
    "    return training_stream, training_raw_dmap, training_smooth_dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNet, self).__init__()\n",
    "\n",
    "        # 2 continous input state variables\n",
    "        state_size = 2\n",
    "        \n",
    "        # 1 continuous output action, mean and sigma (std. dev.) for each\n",
    "        actions_size = 1\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer of action means\n",
    "        self.fc4 = nn.Linear(hidden_size, actions_size)\n",
    "        \n",
    "        # Standard deviations approximated seperately\n",
    "        self.register_parameter('log_sigma', None)\n",
    "        self.log_sigma = nn.Parameter(torch.ones(actions_size), requires_grad=True)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))   \n",
    "        x = F.relu(self.fc3(x))   \n",
    "        \n",
    "        means = F.tanh(self.fc4(x))\n",
    "        sigmas = torch.exp(self.log_sigma).expand(means.shape)\n",
    "        \n",
    "        return means, sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, policy, lr=1e-2):\n",
    "        self.policy = policy\n",
    "        self.optimiser = optim.Adam(policy.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "    def act(self, states):\n",
    "        \n",
    "        # Create a distribution using the output of the network\n",
    "        means, sigmas = self.policy.forward(states)\n",
    "        m = torch.distributions.Normal(means, sigmas)\n",
    "        \n",
    "        # Sample the distribution to select an action (for each agent)\n",
    "        actions = m.sample()\n",
    "        \n",
    "        # Return sampled action along with the log probability\n",
    "        return actions, m.log_prob(actions)\n",
    "    \n",
    "    \n",
    "    def learn(self, per_trajectory_log_prob, per_trajectory_rewards):\n",
    "        \n",
    "        # Normalise return\n",
    "        per_trajectory_return = torch.tensor(per_trajectory_rewards).sum(dim=0)\n",
    "        per_trajectory_return -= per_trajectory_return.mean()\n",
    "        per_trajectory_return /=  per_trajectory_return.std()\n",
    "\n",
    "        # Define loss based on reinforce\n",
    "        per_trajectory_log_prob_sum = torch.stack(per_trajectory_log_prob).squeeze(2).sum(dim=0)\n",
    "        per_trajectory_loss = -per_trajectory_return * per_trajectory_log_prob_sum\n",
    "        loss = torch.mean(per_trajectory_loss)\n",
    "        \n",
    "         # Update model\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, agent, time_max=999):\n",
    "    \n",
    "    per_trajectory_log_prob = []\n",
    "    per_trajectory_rewards = []\n",
    "\n",
    "    # Run concurrent episode on all environments\n",
    "    states = torch.tensor([env.reset() for env in envs]).float()\n",
    "    for time in range(time_max):\n",
    "        \n",
    "        # Calculate all next actions for all envs\n",
    "        actions, log_probs = agent.act(states)\n",
    "        \n",
    "        # Run through the envs in parallel\n",
    "        outcomes = [env.step(action) for env, action in zip(envs, actions)]\n",
    "        next_states, rewards, dones = ([s for s,r,d,_ in outcomes],\n",
    "                                 [r for s,r,d,_ in outcomes],\n",
    "                                 [d for s,r,d,_ in outcomes])\n",
    "        \n",
    "        # Pack up the next_states ready to send back to the agent\n",
    "        states = torch.tensor(next_states).float()\n",
    "        \n",
    "        # Record result\n",
    "        per_trajectory_log_prob.append(log_probs)\n",
    "        per_trajectory_rewards.append(rewards)\n",
    "    \n",
    "        # We want rectangular input to network, so if any finish early we finish all early\n",
    "        if any(dones):\n",
    "            break\n",
    "    \n",
    "    return (per_trajectory_log_prob, per_trajectory_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parallel envs\n",
    "num_envs = 10\n",
    "\n",
    "def make_env(env_id, rank, seed=0):\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(seed + rank)\n",
    "    return env\n",
    "\n",
    "envs = [make_env('MountainCarContinuous-v0', i) for i in range(num_envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and agent\n",
    "policy = PolicyNet()\n",
    "agent = Agent(policy, lr=1e-2)\n",
    "\n",
    "# Create returns record, this will stay consistent across runs\n",
    "time_index = 0\n",
    "average_returns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training progress\n",
    "#%%opts Curve [width=700 height=200 show_grid=True tools=['hover']]\n",
    "stream, smooth, raw = init_training_progress()\n",
    "layout = (smooth * raw)\n",
    "layout.opts(\n",
    "    opts.Curve(width=900, height=300, show_grid=True, tools=['hover'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_max = 1000\n",
    "time_max = 999\n",
    "\n",
    "for i_episode in range(episode_max):\n",
    "    \n",
    "    (per_trajectory_log_prob, per_trajectory_rewards) = collect_trajectories(envs, agent, time_max=time_max)\n",
    "    agent.learn(per_trajectory_log_prob, per_trajectory_rewards)\n",
    "\n",
    "    average_episode_return = np.sum(per_trajectory_rewards) / len(envs)\n",
    "    stream.emit( pd.DataFrame({'x': average_episode_return}, index=[time_index]) )\n",
    "    time_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in policy.parameters():\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play through episode\n",
    "time_max = 999\n",
    "\n",
    "env = envs[0]\n",
    "states = torch.tensor([env.reset()]).float()\n",
    "for i in range(time_max):\n",
    "    \n",
    "    env.render()\n",
    "\n",
    "    # Calculate all next actions for all envs\n",
    "    actions, _ = agent.act(states)\n",
    "\n",
    "    # Run through the envs in parallel\n",
    "    next_state, reward, done, _ = env.step(actions[0])\n",
    "    print(reward)\n",
    "\n",
    "    # Pack up the next_states ready to send back to the agent\n",
    "    states = torch.tensor([next_state]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
