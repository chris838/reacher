{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we solve the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "The algorithm used is [proximal policy optimisation](https://arxiv.org/abs/1707.06347) with [generalised advantage estimation](https://arxiv.org/abs/1506.02438).\n",
    "\n",
    "### Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.autograd as autograd         \n",
    "import torch.nn as nn                     \n",
    "import torch.nn.functional as F           \n",
    "import torch.optim as optim               \n",
    "import torch.distributions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "\n",
    "import streamz\n",
    "import streamz.dataframe\n",
    "\n",
    "import pdb, gym, collections\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from ppo_agent import PPOAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-party code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function from Shangton Zhang's awesome repo\n",
    "# https://github.com/ShangtongZhang/DeepRL\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup\n",
    "\n",
    "We use holoviews to visualise the training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training progress output\n",
    "def init_training_progress():\n",
    "\n",
    "    max_length = 1000000\n",
    "    rolling_size = 100\n",
    "\n",
    "    training_stream = streamz.Stream()\n",
    "    example = pd.DataFrame({'x': [0]}, index=[0])\n",
    "\n",
    "    training_sdf = streamz.dataframe.DataFrame(training_stream, example=example)\n",
    "\n",
    "    training_raw_buffer = Buffer(training_sdf, length=max_length)\n",
    "    training_smooth_buffer = Buffer(training_sdf.x.rolling(rolling_size).median())\n",
    "\n",
    "    training_raw_dmap = hv.DynamicMap(hv.Curve, streams=[training_raw_buffer]).relabel('raw')\n",
    "    training_smooth_dmap = hv.DynamicMap(hv.Curve, streams=[training_smooth_buffer]).relabel('smooth')\n",
    "    \n",
    "    return training_stream, training_raw_dmap, training_smooth_dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_and_learn(env, agent, max_episode_length=1000):\n",
    "    \n",
    "    # Run concurrent episode on all environments\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = torch.from_numpy(env_info.vector_observations).float()\n",
    "    \n",
    "    # Start episode\n",
    "    agent.start_episode()\n",
    "    \n",
    "    for _ in range(max_episode_length):    \n",
    "        \n",
    "        # Calculate actions for all envs\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Run through the envs in parallel\n",
    "        env_info = env.step(action.numpy())[brain_name]\n",
    "        next_state = torch.from_numpy(env_info.vector_observations).float()\n",
    "        reward = torch.tensor(env_info.rewards).float()\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        # Record the experience tuple with the agent\n",
    "        agent.record_outcome(reward)\n",
    "        \n",
    "        # Advance\n",
    "        state = next_state\n",
    "\n",
    "        # We want rectangular input to network, so if any finish early we finish all early\n",
    "        if np.any(done):\n",
    "            print(\"Someone finished\")\n",
    "            break\n",
    "    \n",
    "    # Finalise episode\n",
    "    agent.finish_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the agent\n",
    "\n",
    "The environment is considered solved when we reach an average score of 30 over 100 succesive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and agent\n",
    "state_size = brain.vector_observation_space_size\n",
    "action_size = brain.vector_action_space_size\n",
    "agent = PPOAgent(state_size, action_size, lr=3e-4, rollout_length=500, gamma=0.99, minibatch_size=64)\n",
    "episode_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display live training progress\n",
    "stream, smooth, raw = init_training_progress()\n",
    "layout = (smooth * raw)\n",
    "layout.opts(\n",
    "    opts.Curve(width=900, height=300, show_grid=True, tools=['hover'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "num_episodes = 300\n",
    "max_episode_length = 1000\n",
    "last_100_average_returns = collections.deque(maxlen=100)\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    \n",
    "    play_episode_and_learn(env, agent, max_episode_length=max_episode_length)\n",
    "    \n",
    "    average_episode_return = (agent.average_episode_return() / max_episode_length) * 1000\n",
    "    last_100_average_returns.append(average_episode_return)\n",
    "    score = np.mean(last_100_average_returns)\n",
    "    print(\"Average return over last 100 episodes {}\".format(score), end=\"\\r\")\n",
    "    \n",
    "    stream.emit( pd.DataFrame({'x': average_episode_return}, index=[episode_index]) )\n",
    "\n",
    "    if score > 30:\n",
    "        print(\"Reached target score of 30 in {} episodes\".format(episode_index))\n",
    "        break\n",
    "    \n",
    "    episode_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(agent.policy, 'agent.policy.model')\n",
    "torch.save(agent.value_estimator, 'agent.value_estimator.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "agent.policy = torch.load('agent.policy.model')\n",
    "agent.policy.eval()\n",
    "agent.value_estimator = torch.load('agent.value_estimator.model')\n",
    "agent.value_estimator.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]      \n",
    "num_agents = len(env_info.agents)\n",
    "states = torch.from_numpy(env_info.vector_observations).float()                  \n",
    "while True:\n",
    "    actions = agent.act(states).numpy()\n",
    "    actions = np.clip(actions, -1, 1)                  \n",
    "    env_info = env.step(actions)[brain_name]           \n",
    "    next_states = torch.from_numpy(env_info.vector_observations).float()\n",
    "    dones = env_info.local_done                        \n",
    "    states = next_states                               \n",
    "    if np.any(dones):                                  \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
