{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.autograd as autograd         \n",
    "import torch.nn as nn                     \n",
    "import torch.nn.functional as F           \n",
    "import torch.optim as optim               \n",
    "import torch.distributions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "\n",
    "import streamz\n",
    "import streamz.dataframe\n",
    "\n",
    "import pdb, gym\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function from Shangton Zhang's code\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training progress output\n",
    "def init_training_progress():\n",
    "\n",
    "    max_length = 1000000\n",
    "    rolling_size = 25\n",
    "\n",
    "    training_stream = streamz.Stream()\n",
    "    example = pd.DataFrame({'x': [0]}, index=[0])\n",
    "\n",
    "    training_sdf = streamz.dataframe.DataFrame(training_stream, example=example)\n",
    "\n",
    "    training_raw_buffer = Buffer(training_sdf, length=max_length)\n",
    "    training_smooth_buffer = Buffer(training_sdf.x.rolling(rolling_size).median())\n",
    "\n",
    "    training_raw_dmap = hv.DynamicMap(hv.Curve, streams=[training_raw_buffer]).relabel('raw')\n",
    "    training_smooth_dmap = hv.DynamicMap(hv.Curve, streams=[training_smooth_buffer]).relabel('smooth')\n",
    "    \n",
    "    return training_stream, training_raw_dmap, training_smooth_dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer of action means\n",
    "        self.fc3= nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Standard deviations approximated seperately\n",
    "        self.register_parameter('log_sigma', None)\n",
    "        self.log_sigma = nn.Parameter(torch.zeros(action_size), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        \n",
    "        means = torch.tanh(self.fc3(x))\n",
    "        sigmas = torch.exp(self.log_sigma).expand(means.shape)\n",
    "        \n",
    "        return means, sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNet, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer - single state value\n",
    "        self.fc3= nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rollout():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_rollout()\n",
    "    \n",
    "    def start_rollout(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "        \n",
    "    def record_decision(self, state, action, log_prob):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)        \n",
    "    \n",
    "    def record_outcome(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def flatten_trajectories(self):\n",
    "        \n",
    "        # Create tensors from states and actions\n",
    "        states_tensors = torch.stack(self.states)\n",
    "        actions_tensors = torch.stack(self.actions)\n",
    "        \n",
    "        # Calculate future return (at each step, for each trajectory)\n",
    "        # This is just the cummulative sum of rewards calculated backwards from the episode end\n",
    "        future_returns = np.cumsum(self.rewards[::-1], axis=0)[::-1].copy()\n",
    "        future_returns = torch.tensor(future_returns).float()\n",
    "        \n",
    "        # Normalise future return (at each step, for each trajectory)\n",
    "        mean = future_returns.mean(dim=1).unsqueeze(1)\n",
    "        sigma = future_returns.std(dim=1).unsqueeze(1)\n",
    "        normalised_future_returns = (future_returns - mean) / (sigma + 1.0e-10)\n",
    "\n",
    "        # Sum the log probabilities over the possible actions (at each step, for each trajectory)\n",
    "        # We don't differentiate with respect to these, hence we detach them from the computation graph\n",
    "        original_policy_log_probs = torch.stack(self.log_probs).sum(-1).detach()\n",
    "        original_policy_probs = torch.exp(original_policy_log_probs)\n",
    "        \n",
    "        # Flatten trajectories\n",
    "        return (states_tensors.view(-1, states_tensors.shape[-1]), \n",
    "                actions_tensors.view(-1, actions_tensors.shape[-1]),\n",
    "                normalised_future_returns.view(-1),\n",
    "                original_policy_probs.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr=1e-3, clipping_epsilon=0.1,\n",
    "                 ppo_epochs=10, minibatch_size=32, rollout_length=1000):\n",
    "        self.lr = lr\n",
    "        self.clipping_epsilon = clipping_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rollout_length = rollout_length\n",
    "        \n",
    "        self.actor = PolicyNet(state_size, action_size)\n",
    "        self.critic = ValueNet(state_size)\n",
    "        \n",
    "        self.rollout = Rollout()\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episode_rewards = []\n",
    "        self.rollout.start_rollout()\n",
    "\n",
    "    def act(self, state):\n",
    "        \n",
    "        # Check if the rollout is full and needs processing\n",
    "        if len(self.rollout) == self.rollout_length:\n",
    "            self.learn()\n",
    "            self.rollout.start_rollout()\n",
    "        \n",
    "        # Derive action distribution from the policy\n",
    "        m = self.action_distribution_for_states(state)\n",
    "        \n",
    "        # DELETE ME !!!\n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        \n",
    "        # Record decision and return sampled action\n",
    "        self.rollout.record_decision(state, action, log_prob)\n",
    "        return action\n",
    "    \n",
    "    def finish_episode(self):\n",
    "        self.learn()\n",
    "    \n",
    "    def record_outcome(self, reward):\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.rollout.record_outcome(reward)\n",
    "        \n",
    "    def action_distribution_for_states(self, states):\n",
    "        means, sigmas = self.actor(states)\n",
    "        return torch.distributions.Normal(means, sigmas)\n",
    "    \n",
    "    def average_episode_return(self):\n",
    "        return np.sum([np.mean(r) for r in self.episode_rewards])\n",
    "    \n",
    "    def get_current_policy_probs(self, states, actions):\n",
    "        \n",
    "        # For the given state/action pairs, create a distribution from the policy and get the log probs\n",
    "        current_policy_log_probs = self.action_distribution_for_states(states).log_prob(actions)\n",
    "\n",
    "        # Sum log probs over the possible actions\n",
    "        current_policy_log_probs = current_policy_log_probs.sum(-1)\n",
    "        \n",
    "        return torch.exp(current_policy_log_probs)\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        (states, actions, normalised_future_returns, original_policy_probs) = \\\n",
    "            self.rollout.flatten_trajectories()\n",
    "        \n",
    "        # Run through PPO epochs\n",
    "        optimiser = optim.Adam(self.actor.parameters(), lr=self.lr, eps=1e-5)\n",
    "        for ppo_epoch in range(self.ppo_epochs):\n",
    "            \n",
    "            # Sample the trajectories randomly in mini-batches\n",
    "            for indices in random_sample(np.arange(states.shape[0]), self.minibatch_size):\n",
    "                \n",
    "                # Sample using sample indices\n",
    "                states_sample = states[indices]\n",
    "                actions_sample = actions[indices]\n",
    "                normalised_future_returns_sample = normalised_future_returns[indices]\n",
    "                original_policy_probs_sample = original_policy_probs[indices]\n",
    "            \n",
    "                # Use the current policy to get the probabilities for the sample states and actions\n",
    "                # We use these to weight the likehoods, allowing resuse of the rollout\n",
    "                current_policy_probs_sample = self.get_current_policy_probs(states_sample, actions_sample)\n",
    "\n",
    "                # Define PPO surrogate and clip\n",
    "                sampling_ratio = current_policy_probs_sample / original_policy_probs_sample\n",
    "                clip = torch.clamp(sampling_ratio, 1 - self.clipping_epsilon, 1 + self.clipping_epsilon)\n",
    "                clipped_surrogate = torch.min(\n",
    "                    sampling_ratio * normalised_future_returns_sample,\n",
    "                    clip * normalised_future_returns_sample)\n",
    "\n",
    "                # Average over trajectories and timesteps\n",
    "                # Apparently this is preferable to summing over timesteps, since we normalised our rewards\n",
    "                # (also negative since we want to ascend)\n",
    "                loss = -torch.mean(clipped_surrogate) \n",
    "\n",
    "                # Update model\n",
    "                optimiser.zero_grad()\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_and_learn(env, agent, max_episode_length=1000):\n",
    "    \n",
    "    # Run concurrent episode on all environments\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = torch.from_numpy(env_info.vector_observations).float()\n",
    "    \n",
    "    # Start episode\n",
    "    agent.start_episode()\n",
    "    \n",
    "    for _ in range(max_episode_length):    \n",
    "        \n",
    "        # Calculate actions for all envs\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Run through the envs in parallel\n",
    "        env_info = env.step(action.numpy())[brain_name]\n",
    "        next_state = torch.from_numpy(env_info.vector_observations).float()\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        # Record the experience tuple with the agent\n",
    "        agent.record_outcome(reward)\n",
    "        \n",
    "        # Advance\n",
    "        state = next_state\n",
    "\n",
    "        # We want rectangular input to network, so if any finish early we finish all early\n",
    "        if np.any(done):\n",
    "            print(\"Someone finished\")\n",
    "            break\n",
    "    \n",
    "    # Finalise episode\n",
    "    agent.finish_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "agent = Agent(33, 4, lr=3e-4, rollout_length=3)\n",
    "\n",
    "agent.start_episode()\n",
    "\n",
    "# Dummy interact with env\n",
    "states = np.zeros((20,33))\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    # Get actions\n",
    "    actions = agent.act(torch.from_numpy(states).float())\n",
    "    actions = torch.clamp(actions, -1, 1)\n",
    "    \n",
    "    # Dummy interact with env\n",
    "    next_states = (i+1) * 0.1 * np.ones((20,33))\n",
    "    rewards = 0.1 * np.ones(20)\n",
    "    dones = [False] * 20\n",
    "\n",
    "    # Teach agent\n",
    "    agent.record_outcome(rewards)\n",
    "    pdb.set_trace()\n",
    "    \n",
    "    # Advance\n",
    "    states = next_states\n",
    "\n",
    "agent.finish_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
