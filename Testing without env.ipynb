{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.autograd as autograd         \n",
    "import torch.nn as nn                     \n",
    "import torch.nn.functional as F           \n",
    "import torch.optim as optim               \n",
    "import torch.distributions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "\n",
    "import streamz\n",
    "import streamz.dataframe\n",
    "\n",
    "import pdb, gym\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function from Shangton Zhang's code\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training progress output\n",
    "def init_training_progress():\n",
    "\n",
    "    max_length = 1000000\n",
    "    rolling_size = 25\n",
    "\n",
    "    training_stream = streamz.Stream()\n",
    "    example = pd.DataFrame({'x': [0]}, index=[0])\n",
    "\n",
    "    training_sdf = streamz.dataframe.DataFrame(training_stream, example=example)\n",
    "\n",
    "    training_raw_buffer = Buffer(training_sdf, length=max_length)\n",
    "    training_smooth_buffer = Buffer(training_sdf.x.rolling(rolling_size).median())\n",
    "\n",
    "    training_raw_dmap = hv.DynamicMap(hv.Curve, streams=[training_raw_buffer]).relabel('raw')\n",
    "    training_smooth_dmap = hv.DynamicMap(hv.Curve, streams=[training_smooth_buffer]).relabel('smooth')\n",
    "    \n",
    "    return training_stream, training_raw_dmap, training_smooth_dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer of action means\n",
    "        self.fc3= nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Standard deviations approximated seperately\n",
    "        self.register_parameter('log_sigma', None)\n",
    "        self.log_sigma = nn.Parameter(torch.zeros(action_size), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        \n",
    "        means = torch.tanh(self.fc3(x))\n",
    "        sigmas = torch.exp(self.log_sigma).expand(means.shape)\n",
    "        \n",
    "        return means, sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNet, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer - single state value\n",
    "        self.fc3= nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rollout():\n",
    "    \n",
    "    def __init__(self, gamma=0.99, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.start_rollout()\n",
    "    \n",
    "    def start_rollout(self):\n",
    "        self.states = []\n",
    "        self.state_values = []\n",
    "        self.actions = []\n",
    "        self.action_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "        \n",
    "    def record_decision(self, state, state_value, action, action_log_prob):\n",
    "        self.states.append(state)\n",
    "        self.state_values.append(state_value)\n",
    "        self.actions.append(action)\n",
    "        self.action_log_probs.append(action_log_prob)        \n",
    "    \n",
    "    def record_outcome(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def flatten_trajectories(self):\n",
    "        \n",
    "        # Create tensors from states and actions\n",
    "        states_tensors = torch.stack(self.states)\n",
    "        actions_tensors = torch.stack(self.actions)\n",
    "        \n",
    "        # Calculate future return (at each step, for each trajectory)\n",
    "        future_returns = [None] * len(self.rewards)\n",
    "        future_return = torch.zeros_like(self.rewards[0])\n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            future_return *= self.gamma\n",
    "            future_return += self.rewards[i]\n",
    "            future_returns[i] = future_return.clone()\n",
    "        future_returns = torch.stack(future_returns).detach()\n",
    "\n",
    "        # Calculate TD errors\n",
    "        td_errors = []\n",
    "        state_values = self.state_values + [0] # terminal state has value 0\n",
    "        for i in range(len(self.rewards)):\n",
    "            td_error = self.rewards[i] + (self.gamma * state_values[i+1]) - state_values[i]\n",
    "            td_errors.append(td_error)\n",
    "            \n",
    "        # Calculate advantages\n",
    "        advantages = [None] * len(self.rewards)\n",
    "        advantage = torch.zeros_like(self.rewards[0])\n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            advantage *= self.gae_lambda * self.gamma\n",
    "            advantage += td_errors[i]\n",
    "            advantages[i] = advantage.clone()\n",
    "        advantages = torch.stack(advantages).detach()\n",
    "        \n",
    "        # Normalise advantages (across steps and trajectories)\n",
    "        normalised_advantages = (advantages - advantages.mean()) / advantages.std() \n",
    "\n",
    "        # Sum the log probabilities over the possible actions (at each step, for each trajectory)\n",
    "        # We don't differentiate with respect to these, hence we detach them from the computation graph\n",
    "        original_policy_log_probs = torch.stack(self.action_log_probs).sum(-1).detach()\n",
    "        original_policy_probs = torch.exp(original_policy_log_probs)\n",
    "        \n",
    "        # Flatten trajectories\n",
    "        return (states_tensors.view(-1, states_tensors.shape[-1]), \n",
    "                actions_tensors.view(-1, actions_tensors.shape[-1]),\n",
    "                future_returns.view(-1),\n",
    "                normalised_advantages.view(-1),\n",
    "                original_policy_probs.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr=1e-3, gamma=0.99, clipping_epsilon=0.1,\n",
    "                 ppo_epochs=10, minibatch_size=64, rollout_length=1000, gae_lambda=0.95):\n",
    "        self.lr = lr\n",
    "        self.clipping_epsilon = clipping_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rollout_length = rollout_length\n",
    "        \n",
    "        self.policy = PolicyNet(state_size, action_size)\n",
    "        self.value_estimator = ValueNet(state_size)\n",
    "        self.rollout = Rollout(gamma=gamma, gae_lambda=gae_lambda)\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episode_rewards = []\n",
    "        self.rollout.start_rollout()\n",
    "\n",
    "    def act(self, state):\n",
    "        \n",
    "        # Check if the rollout is full and needs processing\n",
    "        if len(self.rollout) == self.rollout_length:\n",
    "            self.learn()\n",
    "            self.rollout.start_rollout()\n",
    "        \n",
    "        # Derive action distribution from policy network\n",
    "        means, sigmas = self.policy(state)\n",
    "        action_distribution = torch.distributions.Normal(means, sigmas)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_prob = action_distribution.log_prob(action)\n",
    "        \n",
    "        # Derive state value estimate from value network\n",
    "        state_value = self.value_estimator(state).squeeze()\n",
    "        \n",
    "        # Record decision and return sampled action\n",
    "        self.rollout.record_decision(state, state_value, action, action_log_prob)\n",
    "        return action\n",
    "    \n",
    "    def finish_episode(self):\n",
    "        self.learn()\n",
    "    \n",
    "    def record_outcome(self, reward):\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.rollout.record_outcome(reward)\n",
    "    \n",
    "    def average_episode_return(self):\n",
    "        return sum([r.mean().item() for r in self.episode_rewards])\n",
    "    \n",
    "    def get_current_policy_probs(self, states, actions):\n",
    "        \n",
    "        # For the given state/action pairs, create a distribution from the policy and get the log probs\n",
    "        means, sigmas = self.policy(states)\n",
    "        action_distribution = torch.distributions.Normal(means, sigmas)\n",
    "        current_policy_log_probs = action_distribution.log_prob(actions)\n",
    "\n",
    "        # Sum log probs over the possible actions\n",
    "        current_policy_log_probs = current_policy_log_probs.sum(-1)\n",
    "        \n",
    "        return torch.exp(current_policy_log_probs)\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        (states, actions, future_returns, normalised_advantages, original_policy_probs) = \\\n",
    "            self.rollout.flatten_trajectories()\n",
    "        \n",
    "        # Run through PPO epochs\n",
    "        policy_optimiser = optim.Adam(self.policy.parameters(), lr=self.lr, eps=1e-5)\n",
    "        value_estimator_optimiser = optim.Adam(self.value_estimator.parameters(), lr=self.lr, eps=1e-5)\n",
    "        for ppo_epoch in range(self.ppo_epochs):\n",
    "            \n",
    "            # Sample the trajectories randomly in mini-batches\n",
    "            for indices in random_sample(np.arange(states.shape[0]), self.minibatch_size):\n",
    "                \n",
    "                # Sample using sample indices\n",
    "                states_sample = states[indices]\n",
    "                actions_sample = actions[indices]\n",
    "                future_returns_sample = future_returns[indices]\n",
    "                normalised_advantages_sample = normalised_advantages[indices]\n",
    "                original_policy_probs_sample = original_policy_probs[indices]\n",
    "            \n",
    "                # Use the current policy to get the probabilities for the sample states and actions\n",
    "                # We use these to weight the likehoods, allowing resuse of the rollout\n",
    "                current_policy_probs_sample = self.get_current_policy_probs(states_sample, actions_sample)\n",
    "\n",
    "                # Define PPO surrogate and clip to get the policy loss\n",
    "                sampling_ratio = current_policy_probs_sample / original_policy_probs_sample\n",
    "                clipped_ratio = torch.clamp(sampling_ratio, 1 - self.clipping_epsilon, 1 + self.clipping_epsilon)\n",
    "                clipped_surrogate = torch.min(\n",
    "                    sampling_ratio * normalised_advantages_sample,\n",
    "                    clipped_ratio * normalised_advantages_sample)\n",
    "                policy_loss = -torch.mean(clipped_surrogate) \n",
    "\n",
    "                # Define value estimator loss\n",
    "                state_values_sample = self.value_estimator(states_sample).squeeze()\n",
    "                value_estimator_loss = nn.MSELoss()(state_values_sample, future_returns_sample)\n",
    "\n",
    "                # Update value estimator\n",
    "                value_estimator_optimiser.zero_grad()\n",
    "                value_estimator_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.value_estimator.parameters(), 0.75) \n",
    "                value_estimator_optimiser.step()\n",
    "                \n",
    "                # Update policy\n",
    "                policy_optimiser.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.75) \n",
    "                policy_optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "agent = Agent(33, 4, lr=3e-4, rollout_length=3)\n",
    "\n",
    "agent.start_episode()\n",
    "\n",
    "# Dummy interact with env\n",
    "states = torch.zeros((20,33)).float()\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    # Get actions\n",
    "    actions = agent.act(states)\n",
    "    \n",
    "    # Dummy interact with env\n",
    "    next_states = (i+1) * 0.1 * torch.ones((20,33))\n",
    "    rewards = 0.1 * torch.ones(20)\n",
    "    dones = [False] * 20\n",
    "\n",
    "    # Teach agent\n",
    "    agent.record_outcome(rewards)\n",
    "    \n",
    "    # Advance\n",
    "    states = next_states\n",
    "\n",
    "agent.finish_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(states, actions, future_returns, normalised_advantages, original_policy_probs) = \\\n",
    "            agent.rollout.flatten_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_and_learn(env, agent, max_episode_length=1000):\n",
    "    \n",
    "    # Run concurrent episode on all environments\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = torch.from_numpy(env_info.vector_observations).float()\n",
    "    \n",
    "    # Start episode\n",
    "    agent.start_episode()\n",
    "    \n",
    "    for _ in range(max_episode_length):    \n",
    "        \n",
    "        # Calculate actions for all envs\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Run through the envs in parallel\n",
    "        env_info = env.step(action.numpy())[brain_name]\n",
    "        next_state = torch.from_numpy(env_info.vector_observations).float()\n",
    "        reward = torch.tensor(env_info.rewards).float()\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        # Record the experience tuple with the agent\n",
    "        agent.record_outcome(reward)\n",
    "        \n",
    "        # Advance\n",
    "        state = next_state\n",
    "\n",
    "        # We want rectangular input to network, so if any finish early we finish all early\n",
    "        if np.any(done):\n",
    "            print(\"Someone finished\")\n",
    "            break\n",
    "    \n",
    "    # Finalise episode\n",
    "    agent.finish_episode(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create policy and agent\n",
    "#state_size = brain.vector_observation_space_size\n",
    "#action_size = brain.vector_action_space_size\n",
    "#agent = Agent(state_size, action_size, lr=3e-4, rollout_length=500, gamma=0.99, minibatch_size=64)\n",
    "#episode_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display training progress\n",
    "#stream, smooth, raw = init_training_progress()\n",
    "#layout = (smooth * raw)\n",
    "#layout.opts(\n",
    "#    opts.Curve(width=900, height=300, show_grid=True, tools=['hover'])\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "#num_episodes = 50\n",
    "#max_episode_length = 1000\n",
    "#\n",
    "#for _ in range(num_episodes):\n",
    "#    \n",
    "#    play_episode_and_learn(env, agent, max_episode_length=max_episode_length)\n",
    "#    \n",
    "#    average_episode_return = (agent.average_episode_return() / max_episode_length) * 1000\n",
    "#    stream.emit( pd.DataFrame({'x': average_episode_return}, index=[episode_index]) )\n",
    "#    episode_index += 1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
