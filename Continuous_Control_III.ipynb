{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train an agent\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.autograd as autograd         \n",
    "import torch.nn as nn                     \n",
    "import torch.nn.functional as F           \n",
    "import torch.optim as optim               \n",
    "import torch.distributions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "\n",
    "import streamz\n",
    "import streamz.dataframe\n",
    "\n",
    "import pdb, gym\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training progress output\n",
    "def init_training_progress():\n",
    "\n",
    "    max_length = 1000000\n",
    "    rolling_size = 25\n",
    "\n",
    "    training_stream = streamz.Stream()\n",
    "    example = pd.DataFrame({'x': [0]}, index=[0])\n",
    "\n",
    "    training_sdf = streamz.dataframe.DataFrame(training_stream, example=example)\n",
    "\n",
    "    training_raw_buffer = Buffer(training_sdf, length=max_length)\n",
    "    training_smooth_buffer = Buffer(training_sdf.x.rolling(rolling_size).median())\n",
    "\n",
    "    training_raw_dmap = hv.DynamicMap(hv.Curve, streams=[training_raw_buffer]).relabel('raw')\n",
    "    training_smooth_dmap = hv.DynamicMap(hv.Curve, streams=[training_smooth_buffer]).relabel('smooth')\n",
    "    \n",
    "    return training_stream, training_raw_dmap, training_smooth_dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_size = 32\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layer of action means\n",
    "        self.fc3= nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # Standard deviations approximated seperately\n",
    "        self.register_parameter('log_sigma', None)\n",
    "        self.log_sigma = nn.Parameter(-1*torch.ones(action_size), requires_grad=True)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        means = torch.tanh(self.fc3(x))\n",
    "        sigmas = torch.exp(self.log_sigma).expand(means.shape)\n",
    "        \n",
    "        return means, sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rollout():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_episode()\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        \n",
    "    def record_decision(self, state, action, log_prob):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)        \n",
    "    \n",
    "    def record_outcome(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def flatten_trajectories(self):\n",
    "        \n",
    "        # Create tensors from states and actions\n",
    "        states_tensors = torch.stack(self.states)\n",
    "        actions_tensors = torch.stack(self.actions)\n",
    "        \n",
    "        # Calculate future return (at each step, for each trajectory)\n",
    "        # This is just the cummulative sum of rewards calculated backwards from the episode end\n",
    "        future_returns = np.cumsum(self.rewards[::-1], axis=0)[::-1].copy()\n",
    "        future_returns = torch.tensor(future_returns).float()\n",
    "        \n",
    "        # Normalise future return (at each step, for each trajectory)\n",
    "        mean = future_returns.mean(dim=1).unsqueeze(1)\n",
    "        sigma = future_returns.std(dim=1).unsqueeze(1)\n",
    "        normalised_future_returns = (future_returns - mean) / (sigma + 1.0e-10)\n",
    "        \n",
    "        # Sum the log probabilities over the possible actions (at each step, for each trajectory)\n",
    "        # We don't differentiate with respect to these, hence we detach them from the computation graph\n",
    "        original_policy_log_probs = torch.stack(self.log_probs).sum(-1).detach()\n",
    "        original_policy_probs = torch.exp(original_policy_log_probs)\n",
    "        \n",
    "        # Flatten trajectories\n",
    "        return (states_tensors.view(-1, states_tensors.shape[-1]), \n",
    "                actions_tensors.view(-1, actions_tensors.shape[-1]),\n",
    "                normalised_future_returns.view(-1),\n",
    "                original_policy_probs.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr=1e-3, clipping_epsilon=0.1, ppo_epochs=10, minibatch_size=32):\n",
    "        self.lr = lr\n",
    "        self.clipping_epsilon = clipping_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "        self.policy = PolicyNet(state_size, action_size)\n",
    "        self.rollout = Rollout()\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.rollout.start_episode()\n",
    "\n",
    "    def act(self, state):\n",
    "        \n",
    "        # Derive action distribution from the policy\n",
    "        m = self.action_distribution_for_states(state)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        \n",
    "        # Record decision and return sampled action\n",
    "        self.rollout.record_decision(state, action, log_prob)\n",
    "        return action\n",
    "    \n",
    "    def record_outcome(self, reward):\n",
    "        self.rollout.record_outcome(reward)\n",
    "        \n",
    "    def action_distribution_for_states(self, states):\n",
    "        means, sigmas = self.policy.forward(states)\n",
    "        return torch.distributions.Normal(means, sigmas)\n",
    "    \n",
    "    def average_episode_return(self):\n",
    "        return np.sum([np.mean(r) for r in self.rollout.rewards])\n",
    "    \n",
    "    def get_current_policy_probs(self, states, actions):\n",
    "        \n",
    "        # For the given state/action pairs, create a distribution from the policy and get the log probs\n",
    "        current_policy_log_probs = self.action_distribution_for_states(states).log_prob(actions)\n",
    "\n",
    "        # Sum log probs over the possible actions\n",
    "        current_policy_log_probs = current_policy_log_probs.sum(-1)\n",
    "        \n",
    "        return torch.exp(current_policy_log_probs)\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        (states, actions, normalised_future_returns, original_policy_probs) = \\\n",
    "            self.rollout.flatten_trajectories()\n",
    "        \n",
    "        # Run through PPO epochs\n",
    "        optimiser = optim.Adam(self.policy.parameters(), lr=self.lr, eps=1e-5)\n",
    "        for ppo_epoch in range(self.ppo_epochs):\n",
    "            \n",
    "            # Sample the trajectories randomly in mini-batches\n",
    "            for indices in random_sample(np.arange(100), self.minibatch_size):\n",
    "                \n",
    "                # Sample using sample indices\n",
    "                states_sample = states[indices]\n",
    "                actions_sample = actions[indices]\n",
    "                normalised_future_returns_sample = normalised_future_returns[indices]\n",
    "                original_policy_probs_sample = original_policy_probs[indices]\n",
    "            \n",
    "                # Use the current policy to get the probabilities for the sample states and actions\n",
    "                # We use these to weight the likehoods, allowing resuse of the rollout\n",
    "                current_policy_probs_sample = self.get_current_policy_probs(states_sample, actions_sample)\n",
    "\n",
    "                # Define PPO surrogate and clip\n",
    "                sampling_ratio = current_policy_probs_sample / original_policy_probs_sample\n",
    "                clip = torch.clamp(sampling_ratio, 1 - self.clipping_epsilon, 1 + self.clipping_epsilon)\n",
    "                clipped_surrogate = torch.min(\n",
    "                    sampling_ratio * normalised_future_returns_sample,\n",
    "                    clip * normalised_future_returns_sample)\n",
    "\n",
    "                # Average over trajectories and timesteps\n",
    "                # Apparently this is preferable to summing over timesteps, since we normalised our rewards\n",
    "                # (also negative since we want to ascend)\n",
    "                loss = -torch.mean(clipped_surrogate) \n",
    "\n",
    "                # Update model\n",
    "                optimiser.zero_grad()\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_and_learn(env, agent, time_max=1000):\n",
    "    \n",
    "    # Run concurrent episode on all environments\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = torch.from_numpy(env_info.vector_observations).float()\n",
    "    agent.start_episode()\n",
    "    for time in range(time_max):\n",
    "        \n",
    "        # Calculate actions for all envs\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Run through the envs in parallel\n",
    "        env_info = env.step(action.numpy())[brain_name]\n",
    "        next_state = torch.from_numpy(env_info.vector_observations).float()\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        # Record the experience tuple with the agent\n",
    "        agent.record_outcome(reward)\n",
    "        \n",
    "        # Advance\n",
    "        state = next_state\n",
    "\n",
    "        # We want rectangular input to network, so if any finish early we finish all early\n",
    "        if np.any(done):\n",
    "            print(\"Someone finished\")\n",
    "            break\n",
    "    \n",
    "    # Instruct agent to learn from the episode\n",
    "    agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and agent\n",
    "state_size = brain.vector_observation_space_size\n",
    "action_size = brain.vector_action_space_size\n",
    "agent = Agent(state_size, action_size, lr=3e-4)\n",
    "episode_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress\n",
    "stream, smooth, raw = init_training_progress()\n",
    "layout = (smooth * raw)\n",
    "layout.opts(\n",
    "    opts.Curve(width=900, height=300, show_grid=True, tools=['hover'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "num_episodes = 10\n",
    "time_max = 1000\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    \n",
    "    play_episode_and_learn(env, agent, time_max=time_max)\n",
    "    \n",
    "    normalised_return = (agent.average_episode_return() / time_max) * 1000\n",
    "    stream.emit( pd.DataFrame({'x': normalised_return}, index=[episode_index]) )\n",
    "    episode_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show params\n",
    "for x in agent.subagents[0].policy.parameters():\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent in the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]      # reset the environment\n",
    "states = torch.from_numpy(env_info.vector_observations).float()\n",
    "num_agents = len(env_info.agents)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "for i in range(100):\n",
    "    actions = agent.act(states)\n",
    "    env_info = env.step(actions.numpy())[brain_name]\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    states = torch.from_numpy(env_info.vector_observations).float()\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "for i, subagent in enumerate(agent.subagents):  \n",
    "    torch.save(subagent.policy, 'subagent.'+str(i)+'.policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "for i, subagent in enumerate(agent.subagents):  \n",
    "    subagent.policy = torch.load('subagent.'+str(i)+'.policy')\n",
    "    subagent.policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minibatch_indices in random_sample(np.arange(100), 32):\n",
    "    print(minibatch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
